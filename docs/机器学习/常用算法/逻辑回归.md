<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/name_code.png"></div>

# 导读

本文介绍逻辑回归算法。

***持续更新中~***



# 目录

<nav>
<a href='#导读' style='text-decoration:none;font-weight:bolder'>导读</a><br/>
<a href='#目录' style='text-decoration:none;font-weight:bolder'>目录</a><br/>
<a href='#正文' style='text-decoration:none;font-weight:bolder'>正文</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1 分类问题' style='text-decoration:none;${border-style}'>1 分类问题</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1.1 常见的分类问题' style='text-decoration:none;${border-style}'>1.1 常见的分类问题</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1.2 如何建立分类算法' style='text-decoration:none;${border-style}'>1.2 如何建立分类算法</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#2 假说表示' style='text-decoration:none;${border-style}'>2 假说表示</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3 判定边界' style='text-decoration:none;${border-style}'>3 判定边界</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4 代价函数' style='text-decoration:none;${border-style}'>4 代价函数</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5 简化的代价函数与梯度下降' style='text-decoration:none;${border-style}'>5 简化的代价函数与梯度下降</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#6 高级优化' style='text-decoration:none;${border-style}'>6 高级优化</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#7 多分类问题' style='text-decoration:none;${border-style}'>7 多分类问题</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#7.1 多分类的常见例子' style='text-decoration:none;${border-style}'>7.1 多分类的常见例子</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#7.2 一对多分类算法' style='text-decoration:none;${border-style}'>7.2 一对多分类算法</a><br/>
<a href='#总结' style='text-decoration:none;font-weight:bolder'>总结</a><br/>
<a href='#参考文献' style='text-decoration:none;font-weight:bolder'>参考文献</a><br/>
</nav>

# 正文

## 1 分类问题

在分类问题中，我们尝试预测是结果是否属于某一个类。

在二分类问题中，我们将因变量（dependent variable）可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量 <img src="http://latex.codecogs.com/svg.latex?y\in_{}\lbrace0,1\rbrace">，其中 0 表示负向类，1 表示正向类。

如果因变量 <img src="http://latex.codecogs.com/svg.latex?y\in_{}\lbrace0,1,2,\hdots\rbrace">，则属于多分类问题。



### 1.1 常见的分类问题

* 判断一封电子邮件是否是垃圾邮件
* 判断金融交易是否是欺诈
* 判断是否患肿瘤疾病，肿瘤是否是良性



### 1.2 如何建立分类算法

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/分类算法建立.png"></div>

<div align="center"><font size="2">分类算法建立</font></div>

我们通过肿瘤大小判断肿瘤是良性还是恶性的二分类问题中，我们尝试使用线性回归算法进行拟合，即该拟合直线为 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta^Tx">。分类的阈值 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)"> 为 0.5，即 <img src="http://latex.codecogs.com/svg.latex?\begin{aligned}if\,h_{\theta}(x)\ge_{}0.5,predict\,'y=1'\\if\,h_{\theta}(x)\lneq{}0.5,predict\,'y=0'\end{aligned}">

如上图，我们对红色样本进行拟合时，对应红色直线，将样本很好的区分开了。此时，如果又增添了绿色的样本，再次进行拟合，对应绿色直线。我们发现原本被红色拟合直线中的两个恶性肿瘤样本被绿色拟合直线划分为良性肿瘤。**由于某些离群数据，导致原本恶性肿瘤被误判为良性肿瘤**。

此外，线性回归模型，其预测值可能超出 [0, 1] 的范围，并不适合解决分类问题。我们需要一种算法使得 y 的值始终在 0 到 1 之间，该算法就是**逻辑回归**（logistic function）。



## 2 假说表示

逻辑回归模型的假设为 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta^Tx)">，其中 x 表示特征向量，g 表示逻辑函数（或 Sigmoid 函数），公式为 <img src="http://latex.codecogs.com/svg.latex?g(z)=\frac{1}{1+e^{-z}}">。

Sigmoid 函数的图形如下：

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/Sigmoid函数（逻辑函数）.png"></div>

<div align="center"><font size="2">Sigmoid函数（逻辑函数）</font></div>

由 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta^Tx)"> 和 <img src="http://latex.codecogs.com/svg.latex?g(z)=\frac{1}{1+e^{-z}}"> 整合可知 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}">。

<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)"> 的作用是，对于给定的输入参数，输出变量为 1 的估计的可能性（**estimated probability**），即条件概率 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=P(y=1|x;\theta)">。例如，如果对于给定的 x，其 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=0.7">，则表示有 70% 的概率 y 为正向类，相对应的 y 为负向类的概率为 30%。



## 3 判定边界

**决策边界（decision boundary）**可以帮忙我们理解逻辑回归的假设函数在计算什么。

我们根据上述 Sigmoid 函数可知，当 <img src="http://latex.codecogs.com/svg.latex?\theta^Tx\ge_{}0">，预测 y 为 1；当 <img src="http://latex.codecogs.com/svg.latex?\theta^Tx\lneq{}0">，预测 y 为 0。

假设我们有一个分类任务，现使用逻辑回归算法进行拟合，即 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2})">，并且参数 θ 已拟合好，<img src="http://latex.codecogs.com/svg.latex?\theta=\begin{bmatrix}-3\\1\\1\end{bmatrix}">。由上述条件可知，当 <img src="http://latex.codecogs.com/svg.latex?-3+x_{1}+x_{2}\ge_{}0">，预测 y 为 1。<img src="http://latex.codecogs.com/svg.latex?-3+x_{1}+x_{2}=0"> 是以 <img src="http://latex.codecogs.com/svg.latex?x_{1}"> 和 <img src="http://latex.codecogs.com/svg.latex?x_{2}"> 为坐标轴的二维平面上的一条直线，该线就是模型的分界线，将预测值为 1 和 0 的区域分隔开。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/决策边界.png"></div>

<div align="center"><font size="2">决策边界</font></div>

有时，决策边界并非是一条直线，而是非线性的。

对下面的问题使用逻辑回归进行分类，可能是这样的多项式 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{1}^2+\theta_{2}^2)">，并且假设参数 θ 已拟合好，<img src="http://latex.codecogs.com/svg.latex?\theta=\begin{bmatrix}-1\\0\\0\\1\\1\end{bmatrix}">。有上述条件可知，当 <img src="http://latex.codecogs.com/svg.latex?-1+x_{1}^2+x_{2}^2\ge_{}0">，预测 y 为 1；否则，预测 y 为 0。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/非线性决策边界.png"></div>

<div align="center"><font size="2">非线性决策边界</font></div>



## 4 代价函数

如何拟合逻辑回归模型的参数 θ？具体来说，需要定义拟合参数的代价函数，即监督学习问题中的逻辑回归模型的拟合问题。

问题标记如下：

| 标记                                                         | 含义                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| n                                                            | 特征的数量                                                   |
|                                                              | 训练实例数                                                   |
| <img src="http://latex.codecogs.com/svg.latex?x^{(i)}">      | 第 i 个训练实例，是特征矩阵中的第 i 行，是一个**向量**（vector），比如 <img src="http://latex.codecogs.com/svg.latex?x^{(2)}=\begin{bmatrix}x_{0}\\x_{1}\\\vdots\\x_{n}\end{bmatrix}">，其中 <img src="http://latex.codecogs.com/svg.latex?x_{0}=1,y\in\lbrace0,1\rbrace">。 |
| <img src="http://latex.codecogs.com/svg.latex?x_{j}^{(i)}">  | 矩阵中第 i 行第 j 列特征，即第 i 个实例的第 j 个特征，如 <img src="http://latex.codecogs.com/svg.latex?x_{2}^{(2)}=3">。 |
| <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)"> | <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}"> |

上述问题如何选择参数 θ 呢？对于线性回归模型，我们定义的代价函数是所有模型的误差平方和 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2">。理论上，我们可以对逻辑回归模型沿用该定义，但问题在于，当我们将 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}"> 带入该定义的代价函数中，得到的代价函数是一个**非凸函数**（non-convex function）。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/凸函数与非凸函数.png"></div>

<div align="center"><font size="2">凸函数与非凸函数</font></div>

如果是非凸函数，那么意味着代价函数有很多局部最小值，将影响梯度下降算法寻找全局最小值。

线性回归的代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2">。我们重新定义逻辑回归的代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}x^{(i)},y^{(i)})">，其中 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}x^{(i)},y^{(i)})=\begin{cases}-log(h_{\theta}(x))\,if\,y=1\\-log(1-h_{\theta}(x))\,if\,y=0\end{cases}">

<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)"> 与 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)"> 之间的关系如下图所示：

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/逻辑回归的代价函数.png"></div>

<div align="center"><font size="2">逻辑回归的代价函数</font></div>

当 y 为 1 时，<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)"> 为 1，此时 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)"> 为 0。

当 y 为 1 时，<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)\rightarrow_{}0">，此时 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)\rightarrow_{}\infty">。

直观上，若假设函数 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=0">，相当于预测 <img src="http://latex.codecogs.com/svg.latex?P(y=1|x;\theta)=0">，但实际上 y = 1，说明我们预测错误，我们要用一个很大的代价惩罚该算法。例如，算法预测某人肿瘤为恶性的可能性是 0，即该病人完全不可能患有恶性肿瘤，但实际上该病人确实是恶性肿瘤，说明我们非常肯定的事情是错误的，那么需要对该算法予以很大的惩罚。

y 为 0 时的情况与 y 为 1 的情况正好相反。

我们定义了单训练样本的代价函数，凸性分析的内容超出该课程范围，但可以证明我们所选择的代价函数会给我们一个凸优化问题。代价函数 J(θ) 是一个凸函数，并且没有局部最优值。



## 5 简化的代价函数与梯度下降

逻辑回归的代价函数如下：

<div align="center"><img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}x^{(i)},y^{(i)})"></div>

<div align="center"><img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)=\begin{cases}-log(h_{\theta}(x))\,if\,y=1\\-log(1-h_{\theta}(x))\,if\,y=0\end{cases},y\in\lbrace0,1\rbrace"></div>

上述 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)"> 可以做合并为 <img src="http://latex.codecogs.com/svg.latex?Cost(h_{\theta}(x),y)=-y\times_{}log(h_{\theta}(x))-(1-y)\times_{}log(1-h_{\theta}(x))">，将等式带入 <img src="http://latex.codecogs.com/svg.latex?J(\theta)"> 可得 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))">

优化函数：<img src="http://latex.codecogs.com/svg.latex?J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))">

目标：拟合参数 θ，<img src="http://latex.codecogs.com/svg.latex?\mathop{min}_{\theta}J(\theta)">

条件：假设基于给定的新的 x 做预测，并且输出 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}\iff_{}P(y=1|x;\theta)">

上述优化问题，可使用**梯度下降算法**（gradient descent），通用模板如下：

Repeat {

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{j}=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta)">

​		<img src="http://latex.codecogs.com/svg.latex?=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}">

​				<img src="http://latex.codecogs.com/svg.latex?(simulateneously\,update\,all\,\theta_{j})">

}

使用上述公式反复同步更新每一个参数，即用自己减去学习率 α 乘以其微分项。

如果有 n 个特征，也就是说 <img src="http://latex.codecogs.com/svg.latex?\theta=\begin{bmatrix}\theta_{0}\\\theta_{1}\\\vdots\\\theta_{n}\end{bmatrix}">，需要使用 <img src="http://latex.codecogs.com/svg.latex?\theta_{j}=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}"> 来同时更新所有 θ 的值。（**编程技巧：可使用向量化实现对所有 θ 同步更新**）

如果把该更新规则和之前的线性回归的更新规则比较的话，会发现该规则正是我们用线性回归梯度下降。

**那么线性回归和逻辑回归是同一个算法吗？**要回答这个问题，我们观察逻辑回归发生了哪些变化。可以发现，假设的定义发生了变化。

| 线性回归函数                                                 | 逻辑回归函数                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta^Tx=\theta_{0}x_{0}+\theta_{1}x_{1}+\hdots+\theta_{n}x_{n}"> | <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\frac{1}{1+e^{-\theta^Tx}}"> |

因此，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑回归函数的梯度下降和线性回归函数的梯度下降是两个完全不同的东西。

和线性回归使用梯度下降法类似，可以使用同样的方法监控梯度下降确保其收敛。

此外，在线性回归中的特征缩放可以提高梯度下降收敛的速度，同样特征缩放同样适用于逻辑回归。



## 6 高级优化

事实上存在高级优化算法和高级的优化概念，利用这些方法，能够使得相较于使用梯度下降算法具有更快的速度，此类算法更适合于解决大型的机器学习问题，例如，有非常庞大的特征数量。

让我们换个角度看梯度下降，如果有代价函数 J(θ)，而我们想要使其最小化，当输入参数 θ 时，它会计算出两个东西：J(θ) 以及 j 等于 0,1,...,n 时的偏导数。

**优化算法**

* 代价函数 <img src="http://latex.codecogs.com/svg.latex?J(\theta)">，想要 <img src="http://latex.codecogs.com/svg.latex?\mathop{min}_{\theta}J(\theta)">

* 给定 θ，我们有计算下述两项的代码：
  *   <img src="http://latex.codecogs.com/svg.latex?J(\theta)">
  *   <img src="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial\theta_{j}}J(\theta)">

* 梯度下降

  Repeat {

  ​	<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta)">

  }

假设我们完成了实现两件事的代码，那么**梯度下降**所做的事情就是反复执行这些更新。梯度下降算法不是我们唯一可以使用的代码，还有**共轭梯度法**、**BFGS（变尺度法）**和 **L-BFGS（限制变尺度法）**之类的高级优化算法。上述算法的具体细节暂不做深究。

高级优化算法的优劣势

* 优势

  * 不需要手动选择学习率 α

    内部有智能的内部循环，称为线性搜索（line search）算法，可以自动尝试不同的学习率 α，并自动选择一个最好的学习率 α，甚至可以为每次迭代选择不同的学习率 α

  * 通常比梯度下降快

* 劣势

  * 更加复杂



## 7 多分类问题

之前我们谈论的都是二分类问题，如何使用逻辑回归（logistic regression）来解决多分类问题，具体来说，可以通过“**一对多**（one-vs-all）”的分类算法。



### 7.1 多分类的常见例子

电子邮件分类/标签：工作，朋友，家庭，兴趣

药物诊断：未感冒，感冒，流感

天气：晴，多云，雨，雪



### 7.2 一对多分类算法

多分类问题的数据集可能像下面这样

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/二分类问题及多分类问题.png"></div>

<div align="center"><font size="2">二分类问题及多分类问题</font></div>

我们使用三种不同的符号表示三种分类，如何训练一个学习算法来进行分类？

我们已经知道可以使用逻辑回归进行二元分类，对于多份类问题，逻辑回归同样适用，即使用一对多分类算法。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/一对多分类.png"></div>

<div align="center"><font size="2">一对多分类</font></div>

**具体过程如下：**

假设，我们有个训练集，其中有三个类别（三角形，方框，叉叉）。

首先从三角形出发，实际上我们可以创建一个新的“伪”训练集**（1）**，三角形设定为正类，方框和叉叉设定为负类，根据该训练集训练处合适的分类模型，记作 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}^{(1)}(x)">。

接着，从多个类中再抽取一个作为正类，其他作为负类，生成训练集，并训练出分类模型。重复该过程。

然后，得到一系列模型，记作 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}^{(i)}(x)=P(y=i|x;\theta)\quad_{}i=(1,2,\hdots,k)">。

最后，我们对输入变量 x 做预测时，需将所有分类模型都运行一遍，选择最高的结果。



总之，需要训练逻辑回归模型 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}^{(i)}(x)">，其中 i 对应每一个可能的 y = i，最后对新的 x 做预测时，在所有的逻辑回归模型的结果中选择使得 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}^{(i)}(x)"> 最大，即 <img src="http://latex.codecogs.com/svg.latex?\mathop{max}_{i}h_{\theta}^{(i)}(x)"> 对应的 i 作为其分类。



# 总结



# 参考文献

[1] [吴恩达机器学习视频教程笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)