<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/name_code.png"></div>

# 导读

本文介绍正则化相关内容。

***持续更新中~***



# 目录

<nav>
<a href='#导读' style='text-decoration:none;font-weight:bolder'>导读</a><br/>
<a href='#目录' style='text-decoration:none;font-weight:bolder'>目录</a><br/>
<a href='#正文' style='text-decoration:none;font-weight:bolder'>正文</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1 过拟合问题' style='text-decoration:none;${border-style}'>1 过拟合问题</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1.1 过拟合问题案例' style='text-decoration:none;${border-style}'>1.1 过拟合问题案例</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1.2 过拟合解决方法' style='text-decoration:none;${border-style}'>1.2 过拟合解决方法</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#2 代价函数' style='text-decoration:none;${border-style}'>2 代价函数</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3 正则化线性回归' style='text-decoration:none;${border-style}'>3 正则化线性回归</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.1 梯度下降' style='text-decoration:none;${border-style}'>3.1 梯度下降</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.2 正规方程' style='text-decoration:none;${border-style}'>3.2 正规方程</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.2.1 不可逆（奇异）' style='text-decoration:none;${border-style}'>3.2.1 不可逆（奇异）</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4 正则化逻辑回归' style='text-decoration:none;${border-style}'>4 正则化逻辑回归</a><br/>
<a href='#总结' style='text-decoration:none;font-weight:bolder'>总结</a><br/>
<a href='#参考文献' style='text-decoration:none;font-weight:bolder'>参考文献</a><br/>
</nav>

# 正文

## 1 过拟合问题

在应用线性回归和逻辑回归时，它能有效解决许多问题，但将它们应用于特定的机器学习应用时，会遇到**过拟合**（over-fitting）问题，导致模型的预测效果很差。

假设我们有非常多的特征，模型通过训练能够很好地适应训练集（代价函数几乎为 0，即 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2\approx_{}0">），但可能会不能推广到新的数据。

### 1.1 过拟合问题案例

接下来，我们看一个回归的例子：

![回归模型的过拟合问题](https://gitee.com/struggle3014/picBed/raw/master/回归模型的过拟合问题.png)

<div align="center"><font size="2">回归模型的过拟合问题</font></div>

第一个模型是线性模型，存在**欠拟合**问题，不能很好地拟合训练集。第三个模型是四次方模型，存在**过拟合**问题，完美地拟合了训练集所有样本点，但新样本预测的效果可能效果不好。第二个模型是二次方模型，拟合地正好，既很好地拟合了训练集，也具有一定的泛化能力。

同样，分类问题中也存在类似的问题：

![分类模型的过拟合问题](https://gitee.com/struggle3014/picBed/raw/master/分类模型的过拟合问题.png)

<div align="center"><font size="2">分类模型的过拟合问题</font></div>

### 1.2 过拟合解决方法

* 丢弃一些不能帮助我们正确预测的特征。

  * 手动选择
  * 算法选择（**PCA**）

* 正则化（regularization）

  正则化会保留所有特征，减少参数的大小



## 2 代价函数

在回归过拟合问题中，产生过拟合问题的模式为 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^2+\theta_{3}x_{3}^3+\theta_{4}x_{4}^4">，正是由于**高次项导致过拟合**的产生。

一个很自然的想法是，如果能让这些高次项的系数接近 0，那么模型就能很好地拟合训练集数据，同时不失样本外的泛化能力。所以，我们要**一定程度上较少高次项对应 θ 的值，这是正则化的基本方法**。具体地，决定减少 <img src="http://latex.codecogs.com/svg.latex?\theta_{3}"> 和 <img src="http://latex.codecogs.com/svg.latex?\theta_{4}"> 的大小，需要修改代价函数，对 <img src="http://latex.codecogs.com/svg.latex?\theta_{3}"> 和 <img src="http://latex.codecogs.com/svg.latex?\theta_{4}"> 设置一点惩罚（设置其系数为一个较大值，例如 1000）。在尝试最小化代价函数时，将惩罚项也考虑进来，并最终导致选择较小的 <img src="http://latex.codecogs.com/svg.latex?\theta_{3}"> 和 <img src="http://latex.codecogs.com/svg.latex?\theta_{4}">，修改后的代价函数如下 <img src="http://latex.codecogs.com/svg.latex?\mathop{min}_{\theta}\frac{1}{2m}(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})+1000\theta_{3}^2+1000\theta_{4}^2)">

![正则化的直观感受](https://gitee.com/struggle3014/picBed/raw/master/正则化的直观感受.png)

<div align="center"><font size="2">正则化的直观感受</font></div>

通过最小化上述代价函数，选择出的 <img src="http://latex.codecogs.com/svg.latex?\theta_{3}\approx_{}0">，<img src="http://latex.codecogs.com/svg.latex?\theta_{4}\approx_{}0">，即 <img src="http://latex.codecogs.com/svg.latex?\theta_{3}"> 和 <img src="http://latex.codecogs.com/svg.latex?\theta_{4}"> 对预测结果的影响比之前小得多。

假设我们有非常多的特征，我们并不知道哪些特征要惩罚，我们将对**所有特征进行惩罚**，并让代价函数最优化来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：<img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2">，其中 λ 为**正则化参数**（Regularization Parameter）。【注】**根据惯例，我们不对 <img src="http://latex.codecogs.com/svg.latex?\theta_{0}"> 进行惩罚**。经过正则化处理的模型与原模型的可能对比如图所示：

![正则化处理](https://gitee.com/struggle3014/picBed/raw/master/正则化处理.png)

<div align="center"><font size="2">正则化处理</font></div>

若选择的正则化参数 λ 过大，则所有参数都最小化，导致模型退化成 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}">，即图中红色水平直线，造成欠拟合。因此，需要选择合理的 λ 的值，这样才能更好地应用正则化。



## 3 正则化线性回归

正则化线性回归的代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=\frac{1}{2m}(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2)">，目标为 <img src="http://latex.codecogs.com/svg.latex?\mathop{min}_{\theta}J(\theta)">，对于线性回归的求解，我们推导了两种学习算法，梯度下降和正规方程。



### 3.1 梯度下降

由于未对 <img src="http://latex.codecogs.com/svg.latex?\theta_{0}"> 进行正规化，因此梯度下降分为两种情况：

Repeat until convergence {

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}">

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha(\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}-\frac{\lambda}{m}\theta_{j})">

​															<img src="http://latex.codecogs.com/svg.latex?j=(1,2,\hdots,n)">

}

正则化等式二可以规整为：<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}">

未正则化梯度下降为：<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}">

对比上述两等式，可以发现，**正则化**是给参数 <img src="http://latex.codecogs.com/svg.latex?\theta_{j}"> 添加系数 <img src="http://latex.codecogs.com/svg.latex?1-\alpha\frac{\lambda}{m}">，又 α，λ，m 为正数，所以 <img src="http://latex.codecogs.com/svg.latex?1-\alpha\frac{\lambda}{m}1"> 该系数比 1 小，可以把它想成是 0.99。



### 3.2 正规方程

若 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}=\begin{bmatrix}(x^{(1)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}_{m\times_{}(n+1)}">, <img src="http://latex.codecogs.com/svg.latex?y=\begin{bmatrix}y^{(1)}\\\vdots\\y^{(m)}\end{bmatrix}_{m\times_{}1}">，目标 <img src="http://latex.codecogs.com/svg.latex?\mathop{min}_{\theta}J(\theta)">

令 <img src="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial\theta_{j}}J(\theta)=0">，可推导出 <img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^T\mathbf{X}+\lambda\begin{bmatrix}0&&&&\\&1&&\mathbf{O}\\&&\ddots&&\\&\mathbf{O}&&1&\\&&&&1\end{bmatrix})^{-1}\mathbf{X}^Ty">，其中 <img src="http://latex.codecogs.com/svg.latex?\begin{bmatrix}0&&&&\\&1&&\mathbf{O}\\&&\ddots&&\\&\mathbf{O}&&1&\\&&&&1\end{bmatrix}\in_{}R^{(n+1)\times_{}(n+1)}">。



#### 3.2.1 不可逆（奇异）

对于线性回归的正规方程，有时存在**不可逆**（Non-invertibility）的问题。

假设 <img src="http://latex.codecogs.com/svg.latex?m\leq_{}n">，其中 m 为样本数，n 为 特征数，<img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}y">，那么 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^T\mathbf{X}"> 将不可逆或**奇异**（Singular）。

正规化可以将某些不可逆矩阵转换为可逆矩阵，例如当 λ > 0 时，<img src="http://latex.codecogs.com/svg.latex?\theta=(\underbrace{\mathbf{X}^T\mathbf{X}+\lambda\begin{bmatrix}0&&&&\\&1&&&\\&&\ddots&&\\&&&&1&\\&&&&&1\end{bmatrix}}_{invertable})^{-1}\mathbf{X}^Ty">



## 4 正则化逻辑回归

在逻辑回归过拟合问题中，产生过拟合问题的模式为 <img src="https://latex.codecogs.com/svg.latex?h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^2+\theta_{3}x_{1}^2x_{2}+\theta_{4}x_{1}^2x_{2}^2+\theta_{5}x_{1}^2x_{2}^3+\hdots)">，也是由于**高次项导致过拟合**的产生。逻辑回归的代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))">，与线性回归模型的正则化处理方法类似，添加正则化项，得 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^2">

![逻辑回归正则化处理](https://gitee.com/struggle3014/picBed/raw/master/逻辑回归正则化处理.png)

<div align="center"><font size="2">逻辑回归正则化处理</font></div>

逻辑回归正则化对应梯度下降为：

Repeat until convergence {

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}">

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha(\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}-\frac{\lambda}{m}\theta_{j})">

​															<img src="http://latex.codecogs.com/svg.latex?j=(1,2,\hdots,n)">

}

我们对比线性回归的梯度下降过程发现两者竟然一致，但注意此处的 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}"> 的定义发生了改变，<img src="http://latex.codecogs.com/svg.latex?h_{\theta}=\frac{1}{1+e^{-\theta^Tx}}">。



# 总结



# 参考文献

[1] [吴恩达机器学习视频教程笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)