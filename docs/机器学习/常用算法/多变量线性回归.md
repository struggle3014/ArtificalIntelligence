<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/name_code.png"></div>

# 导读

本文介绍多变量线性回归（Linear Regression with Multiple Variables）算法。

***持续更新中~***



# 目录

<nav>
<a href='#导读' style='text-decoration:none;font-weight:bolder'>导读</a><br/>
<a href='#目录' style='text-decoration:none;font-weight:bolder'>目录</a><br/>
<a href='#正文' style='text-decoration:none;font-weight:bolder'>正文</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1 多维特征' style='text-decoration:none;${border-style}'>1 多维特征</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#2 多变量梯度下降' style='text-decoration:none;${border-style}'>2 多变量梯度下降</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#2.1 单变量与多变量梯度下降比对' style='text-decoration:none;${border-style}'>2.1 单变量与多变量梯度下降比对</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3 梯度下降实战' style='text-decoration:none;${border-style}'>3 梯度下降实战</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.1 归一化（特征缩放）' style='text-decoration:none;${border-style}'>3.1 归一化（特征缩放）</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.2 学习率' style='text-decoration:none;${border-style}'>3.2 学习率</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4 特征和多项式回归' style='text-decoration:none;${border-style}'>4 特征和多项式回归</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5 正规方程' style='text-decoration:none;${border-style}'>5 正规方程</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5.1 梯度下降法与正规方程比较' style='text-decoration:none;${border-style}'>5.1 梯度下降法与正规方程比较</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#6 正规方程及不可逆性' style='text-decoration:none;${border-style}'>6 正规方程及不可逆性</a><br/>
<a href='#总结' style='text-decoration:none;font-weight:bolder'>总结</a><br/>
<a href='#参考文献' style='text-decoration:none;font-weight:bolder'>参考文献</a><br/>
</nav>

# 正文

## 1 多维特征

在**单变量线性回归**中分析了房屋大小对房价的回归模型。但一般来说，房价的影响因素有很多，例如，卧室数、楼层、年限等。我们为此需要引入**多变量线性回归**进行建模分析。

| 大小（x） |      | 楼层 | 年限 | 价格以1000$为单位（y） |
| --------- | ---- | ---- | ---- | ---------------------- |
| 2104      | 5    | 1    | 45   | 460                    |
| 1416      | 3    | 2    | 40   | 232                    |
| 1534      | 3    | 2    | 30   | 315                    |
| ...       | ...  | ...  | ...  | ...                    |

增加更多特征后，我们对该回归问题标记如下：

| 标记                                                        | 含义                                                         |
| ----------------------------------------------------------- | ------------------------------------------------------------ |
| n                                                           | 特征的数量                                                   |
| <img src="http://latex.codecogs.com/svg.latex?x^{(i)}">     | 第 i 个训练实例，是特征矩阵中的第 i 行，是一个**向量**（vector），比如 <img src="http://latex.codecogs.com/svg.latex?x^{(2)}=\begin{bmatrix}1416\\3\\2\\40\end{bmatrix}">。 |
| <img src="http://latex.codecogs.com/svg.latex?x_{j}^{(i)}"> | 矩阵中第 i 行第 j 列特征，即第 i 个实例的第 j 个特征，如 <img src="http://latex.codecogs.com/svg.latex?x_{2}^{(2)}=3">。 |
| h                                                           | <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\hdots+\theta_{n}x_{n}">，该公式中有 n+1 个参数和 n 个变量。为了使得公式简洁，引入 <img src="http://latex.codecogs.com/svg.latex?x_{0}=1">，则公式转换为 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\hdots+\theta_{n}x_{n}">，公式可以简化为 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta^{T}X">，其中 <img src="http://latex.codecogs.com/svg.latex?\theta=\begin{bmatrix}\theta_{0}\\\theta_{1}\\\theta_{2}\\\vdots\\\theta_{n}\end{bmatrix}\in_{}R^{n+1}">，<img src="http://latex.codecogs.com/svg.latex?X=\begin{bmatrix}x_{0}\\x_{1}\\x_{2}\\\vdots\\x_{n}\end{bmatrix}\in_{}R^{n+1}">。 |



## 2 多变量梯度下降

**Hypothesis（假设函数）**：<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta^{T}X=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\hdots+\theta_{n}x_{n}">，其中 <img src="http://latex.codecogs.com/svg.latex?x_{0}=1">。

**Parameter（参数）**: <img src="http://latex.codecogs.com/svg.latex?\theta_{0},\theta_{1},\hdots,\theta_{n}">

**Cost Function（代价函数）**: <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1},\hdots,\theta_{n})=\theta^{T}X=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2">

**Gradient descent（梯度下降）**：

​	Repeat {

​		<img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1},\hdots,\theta_{n})">

​							 <img src="http://latex.codecogs.com/svg.latex?(simultaneously\,update\,\theta_{j}\,for\,j=0,\hdots,n">)

​	}

### 2.1 单变量与多变量梯度下降比对

| 单变量梯度下降（n=1）                                        | 多变量梯度下降（n>=1）                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Repeat {<br />    <img src="http://latex.codecogs.com/svg.latex?\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)})-y^{(i)}"><br/><br/>    <img src="http://latex.codecogs.com/svg.latex?\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x^{(i)}"><br /><img src="http://latex.codecogs.com/svg.latex?(simultaneously\,update\,\theta_{j}\,for\,j=0,j=1)"><br />} | Repeat {<br /><img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}"><br />                   <img src="http://latex.codecogs.com/svg.latex?(simultaneously\,update\,\theta_{j}\,for\,j=0,\hdots,n)"><br />}<br /><img src="http://latex.codecogs.com/svg.latex?\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}">，其中 <img src="http://latex.codecogs.com/svg.latex?x_{0}^{(i)}=1"><br/><img src="http://latex.codecogs.com/svg.latex?\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)})-y^{(i)})x_{1}^{(i)}"><br/><br/><img src="http://latex.codecogs.com/svg.latex?\theta_{2}:=\theta_{2}-\alpha\frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)})-y^{(i)})x_{2}^{(i)}"><br/><img src="http://latex.codecogs.com/svg.latex?\hdots"> |



## 3 梯度下降实战

### 3.1 归一化（特征缩放）

在面对多维问题时，要保证这些**特征具有近似的尺度**，将有助于梯度下降算法更快地收敛。

以房价问题为例，假设我们使用两个特征，如房屋的尺寸和房间的数量，尺寸的值为 0-2000 平方英尺，而房间数量为 0-5。以上述两个参数作为横纵坐标，绘制代价函数的等高线图，可以看出图像会显得很扁，梯度下降算法需要经过许多轮的迭代才能收敛。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数等高线图.png"></div>

<div align="center"><font size="2">代价函数等高线图</font></div>

解决方案是尝试将所有特征缩放到 -1 到 1 之间。如下图：

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数等高线图（特征尺度不同）.png"></div>

<div align="center"><font size="2">代价函数等高线图（特征尺度不同）</font></div>

* 最大最小值归一化
  * 公式：<img src="http://latex.codecogs.com/svg.latex?\frac{data-min}{max-min}">
  * 缺点：易受离群值的影响
* 方差归一化
  * 公式：<img src="http://latex.codecogs.com/svg.latex?x_{n}=\frac{x_{n}-\mu_{n}}{s_{n}}">，其中 <img src="http://latex.codecogs.com/svg.latex?\mu_{n}"> 是均值，<img src="http://latex.codecogs.com/svg.latex?s_{n}">  是标准差。



### 3.2 学习率

<div align="center"><img src="http://latex.codecogs.com/svg.latex?\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta)"></div>

* “调试”，如何确保梯度下降正确工作？

  <div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/迭代次数与minJ(θ)关系.png"></div>

  <div align="center"><font size="2">迭代次数与minJ(θ)关系</font></div>

  * J(θ)应该随着每次迭代呈现递减趋势。

* 如何确定算法何时收敛

  * 图表法（prefer）

    绘制迭代次数与代价函数图标，如果两次迭代之间的变化趋于一条水平直线，则可知算法已收敛。如上图的绿×和蓝×的变化趋于水平线，则在300左右迭代时，该梯度下降算法收敛。

  * 经验法

    如果某次迭代 J(θ) 下降小于 <img src="http://latex.codecogs.com/svg.latex?10^{-3}"> 则称达到收敛。

* α 大小对代价函数 J(θ) 的影响

  * α 过小

    J(θ) 缓慢收敛，迭代次数非常高

  * α 过大

    每次迭代 J(θ) 可能不会减小，甚至会越过局部最小值导致无法收敛。

* 如何选择学习率 α？

  可以考虑 3 倍递增，如 0.01, 0.03, 0.1, 0.3, 1, 3, 10。绘制迭代次数与代价函数的关系图，判断其关系，确定 α。



## 4 特征和多项式回归

在房价预测问题中，

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/房价预测问题.png"></div>

<div align="center"><font size="2">房价预测问题</font></div>

<img src="http://latex.codecogs.com/svg.latex?\theta_{0}+\theta_{1}*frontage+\theta_{2}*depth">，其中 <img src="http://latex.codecogs.com/svg.latex?x_{1}=frontage">（临街宽度），<img src="http://latex.codecogs.com/svg.latex?x_{2}=depth">（纵向宽度），<img src="http://latex.codecogs.com/svg.latex?x=frontage*depth=area">（面积），则 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}+\theta_{1}x">。

**线性回归并不能适用于所有数据**，有时需要曲线来拟合数据。比如，一个二次方模型 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}"> 或三次方模型 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}^{2}+\theta_{3}x_{3}^{3}">。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/多项式回归.png"></div>

<div align="center"><font size="2">多项式回归</font></div>

如果使用二次多项式 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^2"> 拟合样本数据，拟合的不错，但随着 Size(x) 增大，Price(x) 却在减少，不太符合常理；如果使用三次多项式 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^2+\theta_{3}x^3"> 拟合样本数据，拟合效果还行，但在 Size(x) 增加到一定程度时，随着 Size(x) 增大，Price(x) 理论上不会存在太大变化，而三次多项式仍然呈现快速上趋势，不太符合实际；如果使用带有根号 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}\sqrt{x}"> 的多项式，随着 Size(x) 增大，Price(x) 增大，Size(x) 增大到一定程度时，Price(x) 也逐渐趋于稳定，较为符合实际。

注意：**采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要**。因为特征的数值数量级可能有很大不同。



## 5 正规方程

目前为止，我们使用梯度下降算法求解函数最小值问题，但对于某些线性回归问题，正规方程却是更好的解决方案。

例如，如果未知参数 θ 是一维的，即 <img src="http://latex.codecogs.com/svg.latex?\theta\in_{}R">，代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta)=a\theta^2+b\theta+c">。数值求解最小值，只要将导数设置为0，可求解未知参数 θ，即 <img src="http://latex.codecogs.com/svg.latex?\frac{d}{d_{\theta}}J(\theta)=2a\theta+b=0">，求解 θ。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数数值求解法.png"></div>

<div align="center"><font size="2">代价函数数值求解法</font></div>

同理，如果参数 θ 是 n+1 维向量，即 <img src="http://latex.codecogs.com/svg.latex?\theta\in_{}R^{n+1}">，代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1},\hdots,\theta_{m})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2">。数值求解代价函数最小时参数 θ 的解，只要将各参数偏导数设置为0，联立方程求解，即 <img src="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial\theta_{j}}J(\theta)=\hdots=0\quad(for\,every\,j)">，求解 <img src="http://latex.codecogs.com/svg.latex?\theta_{0},\theta_{1},\hdots,\theta_{n}">。

**具体地**，如果训练样本数 m 为 4。

|                                                       | <img src="http://latex.codecogs.com/svg.latex?Size(feet^2)"> | Num of bedrooms                                       | Num of floors                                         | Age of home(years)                                    | Price($1000) |
| ----------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------- | ----------------------------------------------------- | ------------ |
| <img src="http://latex.codecogs.com/svg.latex?x_{0}"> | <img src="http://latex.codecogs.com/svg.latex?x_{1}">        | <img src="http://latex.codecogs.com/svg.latex?x_{2}"> | <img src="http://latex.codecogs.com/svg.latex?x_{3}"> | <img src="http://latex.codecogs.com/svg.latex?x_{4}"> | y            |
| 1                                                     | 2104                                                         | 5                                                     | 1                                                     | 45                                                    | 460          |
| 1                                                     | 1416                                                         | 3                                                     | 2                                                     | 40                                                    | 232          |
| 1                                                     | 1534                                                         | 3                                                     | 2                                                     | 30                                                    | 315          |
| 1                                                     | 852                                                          | 2                                                     | 1                                                     | 36                                                    | 178          |

则 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}=\begin{bmatrix}1&2104&5&1&45&460\\1&1416&3&2&40&232\\&1534&3&2&30&315\\1&852&2&1&36&178\end{bmatrix}">，<img src="http://latex.codecogs.com/svg.latex?y=\begin{bmatrix}460\\232\\315\\178\end{bmatrix}">，<img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}y">。

**一般地**，假设我们的训练集特征矩阵为 **X**（包含 <img src="http://latex.codecogs.com/svg.latex?x_{0}=1">），训练集结果为向量 y，则利用正规方程求解出 <img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}y">。其中，上标 T 代表矩阵转置，上标 -1 代表矩阵的逆。

m 个训练集样本 <img src="http://latex.codecogs.com/svg.latex?(x^{(1)},y^{(1)}),\hdots,(x^{(m)},y^{(m)})">，n 个特征，其中，<img src="http://latex.codecogs.com/svg.latex?x^{(i)}=\begin{bmatrix}x_{0}^{(i)}\\x_{1}^{(i)}\\\vdots\\x_{n}^{(i)}\end{bmatrix}">，则 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}=\begin{bmatrix}(x^{(1)})^T\\(x^{(2)})^T\\\vdots\\(x^{(m)})^T\end{bmatrix}">，**X** 又称为设计矩阵（degsin matrix），是 m x (n + 1) 维的。

注：对于不可逆的矩阵（通常是特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸；或特征数量大于训练集数量），正规方程是不能使用的。



### 5.1 梯度下降法与正规方程比较

**梯度下降**

* 需选择学习率 α
* 需多次迭代
* 当特征数量很大时，也能很好适用
* 适用于各种类型的模型

**正规方程**

* 一次运算得出
* 需计算 <img src="http://latex.codecogs.com/svg.latex?(\mathbf{X}^T\mathbf{X})^{-1}">，如果特征数较大，则运算代价大，因为逆矩阵计算的时间复杂度为 <img src="http://latex.codecogs.com/svg.latex?O(n^3)">，通常当 n 小于 10000 时是可接受的。
* 只使用线性模型，不适用逻辑回归等复杂模型



## 6 正规方程及不可逆性

正规方程 <img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty">，如果 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^T\mathbf{X}"> 不可逆，我们称不可逆的矩阵为奇异或退化矩阵。

如何解决 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^T\mathbf{X}"> 不可逆问题？

* 冗余特征（线性依赖）
  * 如房价预测问题中同时包含英尺为单位的尺寸和米为单位的尺寸的特征，因为“米”和“英尺”是线性关系 1m=3.28 feet。

* 特征数太多（样本数 <= 特征数量）
  * 删除不重要的特征，用尽可能少的特征来反映更多内容，或使用正则化
* 伪逆函数
  * 如 Octave 中，使用伪逆函数 pinv() 实现。即使 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^T\mathbf{X}"> 不可逆，但算法执行流程正确。



<img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty"> 推导过程

代价函数 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1},\hdots,\theta_{n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2">，其中 <img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x)=\theta^Tx=\theta_{0}x_{0}+\theta_{1}x_{1}+\hdots+\theta_{n}x_{n}">

正规方程是通过求解下面的方程找到代价函数最小的参数，即 <img src="http://latex.codecogs.com/svg.late?\frac{\partial}{\partial\theta_{j}}J(\theta_{j})=0">

设有 m 个实例，每个实例有 n 个特征，则样例为：<img src="http://latex.codecogs.com/svg.latex?\mathbf{X}=\begin{bmatrix}x_{0}^{(1)}&\hdots&x_{n}^{(1)}\\\vdots&\ddots&\vdots\\x_{0}^{(m)}&\hdots&x_{n}^{(m)}\end{bmatrix}">，其中 <img src="http://latex.codecogs.com/svg.latex?x_{j}^{(i)}"> 表示第 i 个实例第 j 个特征，特征参数为 <img src="http://latex.codecogs.com/svg.latex?\theta=\begin{bmatrix}\theta_{0}\\\theta_{1}\\\vdots\\\theta_{n}\end{bmatrix}">，输出变量为 <img src="http://latex.codecogs.com/svg.latex?y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}\end{bmatrix}">

【**解**】

<img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1},\hdots,\theta_{n})=\frac{1}{2m}(\mathbf{X}*\theta-y)^T(\mathbf{X}*\theta-y)">

<img src="http://latex.codecogs.com/svg.latex?=\frac{1}{2m}(\theta^T*\mathbf{X}^T-y^T)(\mathbf{X}*\theta-y)">

<img src="http://latex.codecogs.com/svg.latex?=\frac{1}{2m}(\theta^T*\mathbf{X}^T\mathbf{X}\theta-\theta^T\mathbf{X}^Ty-y^T\mathbf{X}\theta+y^Ty)">

对上式进行求导，等价形式如下

<img src="http://latex.codecogs.com/svg.latex?\frac{1}{2m}(\frac{\partial_{}\theta^TX^TX\theta}{\partial_{}\theta}-\frac{\partial_{}\theta^TX^Ty}{\partial_{}\theta}-\frac{\partial_{}y^TX\theta}{\partial_{}\theta}+\frac{\partial_{}y^Ty}{\partial_{}\theta})">

第一项： <img src="http://latex.codecogs.com/svg.latex?\theta^T\mathbf{X}^T\mathbf{X}\theta"> 是标量，所以标量对向量求导得 <img src="http://latex.codecogs.com/svg.latex?2\mathbf{X}^T\mathbf{X}\theta">

第二项：同样是对标量求导，得 <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^Ty">

第三项： <img src="http://latex.codecogs.com/svg.latex?\mathbf{X}^Ty">

第四项：0

综上，求导结果为 <img src="http://latex.codecogs.com/svg.latex?-2X^{T}y+\mathbf{X}^{T}\mathbf{X}\theta">，为了使代价函数取最小值，令导数为零，得 <img src="http://latex.codecogs.com/svg.latex?\theta=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty">。

【**注**】

1，消除累加符号：假设向量 <img src="http://latex.codecogs.com/svg.latex?\mathbf{A}=\begin{bmatrix}a\\b\\c\\d\end{bmatrix}">，则 <img src="http://latex.codecogs.com/svg.latex?\mathbf{A}^T*\mathbf{A}=\begin{bmatrix}a&b&c&d\end{bmatrix}*\begin{bmatrix}a\\b\\c\\d\end{bmatrix}=\sum_{i=1}^{4}A(i)^{2}">



# 总结

本文介绍多变量线性回归相关的知识，包括梯度下降，特征和多项式回归，正规方程以及正规方程及不可逆性。



# 参考文献

[1] [吴恩达机器学习视频教程笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)

[2] [Matrix Cookbook](http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)

[3] [矩阵求导，CSDN，lx青萍之末](https://blog.csdn.net/daaikuaichuan/article/details/80620518)