<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/name_code.png"></div>

# 导读

本文介绍线性回归（Linear Regression）算法。

***持续更新中~***



# 目录

<nav>
<a href='#导读' style='text-decoration:none;font-weight:bolder'>导读</a><br/>
<a href='#目录' style='text-decoration:none;font-weight:bolder'>目录</a><br/>
<a href='#正文' style='text-decoration:none;font-weight:bolder'>正文</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#1 模型表示' style='text-decoration:none;${border-style}'>1 模型表示</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#2 代价函数' style='text-decoration:none;${border-style}'>2 代价函数</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3 代价函数的直观理解' style='text-decoration:none;${border-style}'>3 代价函数的直观理解</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.1 简化代价函数方式' style='text-decoration:none;${border-style}'>3.1 简化代价函数方式</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#3.2 等高线方式' style='text-decoration:none;${border-style}'>3.2 等高线方式</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4 梯度下降' style='text-decoration:none;${border-style}'>4 梯度下降</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4.1 梯度下降思想' style='text-decoration:none;${border-style}'>4.1 梯度下降思想</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#4.2 梯度下降公式及使用' style='text-decoration:none;${border-style}'>4.2 梯度下降公式及使用</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5 梯度下降的直观理解' style='text-decoration:none;${border-style}'>5 梯度下降的直观理解</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5.1 学习率的大小对于参数调整的影响' style='text-decoration:none;${border-style}'>5.1 学习率的大小对于参数调整的影响</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5.2 学习率 α 保持不变时，梯度下降法如何收敛到局部低点' style='text-decoration:none;${border-style}'>5.2 学习率 α 保持不变时，梯度下降法如何收敛到局部低点</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5.3 参数 θ 初始于局部最低点，梯度下降法如何工作' style='text-decoration:none;${border-style}'>5.3 参数 θ 初始于局部最低点，梯度下降法如何工作</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href='#5.4 从不同的点出发，可能会得到不同的局部最优点' style='text-decoration:none;${border-style}'>5.4 从不同的点出发，可能会得到不同的局部最优点</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;<a href='#6 梯度下降的线性回归' style='text-decoration:none;${border-style}'>6 梯度下降的线性回归</a><br/>
<a href='#总结' style='text-decoration:none;font-weight:bolder'>总结</a><br/>
<a href='#参考文献' style='text-decoration:none;font-weight:bolder'>参考文献</a><br/>
</nav>


# 正文

## 1 模型表示

由预测房屋住房价格的监督学习过程引出线性回归模型。

假设存在一数据集，该数据集包含某市的住房价格。在这里，根据房屋尺寸所售出的价格，画出数据集。例如，如果你朋友的房子是1250平方尺大小，你要告诉他这个房子能卖多少钱。那么，你可以构建一个模型，也许是这条直线，从该数据模型上，告诉你的朋友，他大约能以220000（美元）左右的价格卖掉房子。这就是监督学习算法的例子。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/监督学习（房屋价格预测模型）.png"></div>

<div align="center"><font size="2">监督学习（房屋价格预测模型）</font></div>

上述案例被称为**监督学习**是因为对于每个数据来说，我们都给出了“正确的答案”，这是一个**回归问题**，即根据之前的数据预测一个准确的输出值。同时，还有一类常见的监督学习方式，叫做**分类问题**，预测离散的输出值，例如确定癌症的良性或恶性。

在监督学习中，我们有一个数据集，该数据集被成为**训练集（Training Set）**。以房屋交易问题为例，假设我们回归问题的训练集如下所示：

| 英尺大小（x） | 价格以1000$为单位（y） |
| ------------- | ---------------------- |
| 2104          | 460                    |
| 1416          | 232                    |
| 1534          | 315                    |
| ...           | ...                    |

该回归问题标记如下：

| 标记                                                         | 含义                                               |
| ------------------------------------------------------------ | -------------------------------------------------- |
| m                                                            | 训练集中实例的数量                                 |
| x                                                            | 特征/输出变量                                      |
| y                                                            | 目标变量/输出变量                                  |
| (x, y)                                                       | 训练集中的实例                                     |
| <img src="http://latex.codecogs.com/svg.latex?(x^{(i)}, y^{(i)})"> | 第 i 个观测实例                                    |
| h                                                            | 学习算法的解决方案或函数，也称为假设（hypothesis） |

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/监督学习的工作方式.png"></div>

<div align="center"><font size="2">监督学习算法的工作方式</font></div>

我们的训练集（Training Set）喂给了我们的学习算法，学习算法工作完输出一个函数，用 h 表示。h 代表 hypothesis（假设），h 是从 x 到 y 的函数映射。

对于房价预测问题，如何表达 h？可能的表达方式为：<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x) = \theta_{0} + \theta_{1}x">



## 2 代价函数

假设我们的学习算法的形式为：<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x) = \theta_{0} + \theta_{1}x">，如何为模型选择合适的**参数（parameter）**<img src="http://latex.codecogs.com/svg.latex?\theta_{0}">和<img src="http://latex.codecogs.com/svg.latex?\theta_{1}">，在房价问题的例子中分别对应直线的斜率和 y 轴的截距。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数参数选择.png"></div>

<div align="center"><font size="2">代价函数函数选择</font></div>

参数（parameter）<img src="http://latex.codecogs.com/svg.latex?\theta_{0}">和<img src="http://latex.codecogs.com/svg.latex?\theta_{1}">决定了训练的直线对于训练集的准确程度，模型预测值和实际值之间的差距是**建模误差（modeling error）**。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/建模误差图例.png"></div>

<div align="center"><font size="2">建模误差图例</font></div>

我们的目标是选择可以使得**建模误差平方和最小**的模型参数。用数学公式可表示为 <img src="http://latex.codecogs.com/svg.latex?minnimize_{\theta_{0}, \theta_{1}} \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-(y^{(i)}))^2">，其中 m 表示训练集的数据量，<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x^{(i)}) = \theta_{0} + \theta_{1}x^{(i)}">，1/m 是取平均误差，1/2m 是为了求导时方便。

误差取平方对于大多数问题，特别是回归问题是常用手段，**误差取平方能够更好地考虑离群值的情况**，因为离群值相对于非离群值而言，其误差一般比较大。

**代价函数**一般用 J 表示，即 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-(y^{(i)}))^2">，即 <img src="http://latex.codecogs.com/svg.latex?minnimize_{\theta_{0}, \theta_{1}} J(\theta_{0},\theta_{1})">

我们绘制一个等高图，三个坐标分别为 <img src="http://latex.codecogs.com/svg.latex?\theta_{0}"> 和 <img src="http://latex.codecogs.com/svg.latex?\theta_{1}"> 和 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1})">：

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/损失函数等高线.png"></div>

<div align="center"><font size="2">损失函数等高线</font></div>

则可以看出在三维空间中存在一个使得 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1})"> 最小的点。

代价函数也被成为平方误差函数，也称为平方误差代价函数。平方误差代价函数式解决回归问题最常用的手段。



## 3 代价函数的直观理解

Hypothesis（假设函数）：<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x) = \theta_{0} + \theta_{1}x">

Parameter: <img src="http://latex.codecogs.com/svg.latex?\theta_{0}, \theta_{1}">

Cost Function: <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1}) = \frac{1}{2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-(y^{(i)}))^2">

Goal: <img src="http://latex.codecogs.com/svg.latex?minnimize_{\theta_{0}, \theta_{1}}J(\theta_{0},\theta_{1})">



### 3.1 简化代价函数方式

为了更好地可视化和理解代价函数 J，我们使用一个简化的 Hypothsis（假设函数），<img src="http://latex.codecogs.com/svg.latex?h_{\theta}(x) = \theta_{1}x">。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数直观理解.png"></div>

<div align="center"><font size="2">代价函数直观理解</font></div>



### 3.2 等高线方式

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数图形化.png"></div>

<div align="center"><font size="2">代价函数图形化</font></div><font size="2">

从等高线图可以得知三维空间中存在一个使得 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1})"> 最小的点，即 <font color="green" size="5">×</font> 标记的点，对应的假设函数如绿线斜线所示，从图形中可以直观看出该直线拟合的效果较好。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/代价函数等高线图直观理解.png"></div>

<div align="center"><font size="2">代价函数等高线图直观理解</font></div><font size="2">

当我们遇到更复杂、更高维、更多参数情况下，很难画图，因此也无法将其可视化，我们需要一种有效的算法，能够自动地找出最小化代价函数的 θ 值。



## 4 梯度下降

**梯度下降法**是一种用来**求解函数最小值的算法**，我们将使用梯度下降算法来求解代价函数 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{0},\theta_{1})"> 的最小值。



### 4.1 梯度下降思想

**梯度下降背后的思想**是：开始我们随机选择一个参数的组合 <img src="http://latex.codecogs.com/svg.latex?(\theta_{0},\theta_{1},...,\theta_{n})"> 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值（local minimum），由于我们没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否是全局最小值（global minimum），选择不同的初始参数组，可能会得到不同的局部最小值。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/梯度下降法算法思想.png"></div>

<div align="center"><font size="2">梯度下降算法思想</font></div><font size="2">

想象你正站在山的一点上，在梯度下降算法中，我们要做的就是旋转360度，看看我们周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡的该点上，你看下四周，你会发现最佳的下山方向，你再看看周围，然后再一次想想，应该从什么方向迈小碎步下山？不断重复上述步骤，直到接近局部最低点的位置。**如果我们从不同的点出发可能最终会得到不同的局部最低点，因为可能会存在多个局部最低点**。



### 4.2 梯度下降公式及使用

**批量梯度下降（batch gradient descent）算法公式**：

repeat until convergence { // 重复下述过程，直到收敛

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad (for \, j=0, \, j=1)">

}

其中，**α 是学习率**（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出多大的步子，学习率 α 是正数。在批量梯度下降中，**每一次都<font color="red">同时</font>让所有参数去减去学习率乘以代价函数的导数**。:= 是赋值操作。

**所有参数同时更新**，对于上述公式当 j=0, j=1 时，更新过程如下：

​	<img src="http://latex.codecogs.com/svg.latex?\\ temp0 := \theta_{0} - \alpha \frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \\ temp1 := \theta_{1} - \alpha \frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1})\\ \theta_{0} := temp0 \\ \theta_{1} := temp1">

当谈及梯度下降算法时，就是对所有参数同时更新。同步更新是一种更自然的方式。不同时更新，那是其他算法，此处不做说明。



## 5 梯度下降的直观理解

梯度下降算法：<img src="http://latex.codecogs.com/svg.latex?\theta_{j} := \theta_{j} - \alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})"> 。即对于 θ 赋值，使得 <img src="http://latex.codecogs.com/svg.latex?J(\theta)"> 按照梯度下降最快的方向进行，一直迭代下去，最终得到局部最小值。其中 α 是学习率（learning rate），它决定了我们沿着代价函数下降程度最大的方向向下迈出的步子有多大。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/梯度下降法参数调整方向.png"></div>

<div align="center"><font size="2">梯度下降法参数调整方向</font></div><font size="2">

### 5.1 学习率的大小对于参数调整的影响

* 学习率 α 太小

  由于 α 太小，每次只能调整一点点，需要很多步才能到达局部最低点。**随着慢慢接近局部最低点，导数也越来越小，逼近于0，因此每次调整的步长也越来越小**。

* 学习率 α 太大

  由于 α 太大，可能会越过最低点，甚至无法收敛。

  * 未越过最低点

    与学习率太小同等效果

  * 越过最低点

    * 盘旋下降（**收敛**）
    * 盘旋上升（**发散**）

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/学习率对参数调整的影响.png"></div>

<div align="center"><font size="2">学习率对参数调整的影响</font></div><font size="2">

### 5.2 学习率 α 保持不变时，梯度下降法如何收敛到局部低点

让我们看个例子，代价函数为 <img src="http://latex.codecogs.com/svg.latex?J(\theta_{1})">,梯度下降法的收敛过程。

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/梯度下降可以收敛到局部最小值，即使学习率α固定.png"></div>

<div align="center"><font size="2">梯度下降可以收敛到局部最小值，即使学习率α固定</font></div><font size="2">

我们可以接近局部最小值，随着越来越接近局部最小值，梯度下降会自动调整为越来越小的步子。因此随着时间，无需减少 α 的值。

我们想找它的最小值，首先初始化我们梯度下降算法，在品红色的点出发。如果我们更新一步梯度下降，它也许会把我们带到橙色的点，该点是相当陡，对应的导数绝对值较大。再更新一步梯度下降，走的步数较大，它也许把我们带到红色的点，该点相对于橙色没那么陡。随着我们越来越接近最低点，导数越来越接近于零，所以，梯度下降一步之后，新的导数会变小一点点。重复上述过程，直到收敛到局部最小值。



### 5.3 参数 <img src="http://latex.codecogs.com/svg.latex?\theta_{1}"> 初始于局部最低点，梯度下降法如何工作

由于处于局部最优点，那么局部最优点导数等于0，<img src="http://latex.codecogs.com/svg.latex?\theta_{1}">. 将不再变化，因为新的 <img src="http://latex.codecogs.com/svg.latex?\theta_{1}"> 等于原来的 <img src="http://latex.codecogs.com/svg.latex?\theta_{1}">。



### 5.4 从不同的点出发，可能会得到不同的局部最优点

<div align="center"><img src="https://gitee.com/struggle3014/picBed/raw/master/不同点出发使用梯度下降，可能会得到不同的局部最优点.png"></div>

<div align="center"><font size="2">不同的点出发使用梯度下降，可能会得到不同的局部最优点</font></div><font size="2">



## 6 梯度下降的线性回归

梯度下降算法和线性回归算法比较如下：

**梯度下降算法：**

repeat until convergence {

​	<img src="http://latex.codecogs.com/svg.latex?\\\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\\\qquad (for\,j=0,\,j=1)">

}

**线性回归模型：**

<img src="http://latex.codecogs.com/svg.latex?h_{\theta_{x}}=\theta_{0}+\theta_{1}x\\J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2">



对我们之前的**线性回归问题运用梯度下降法**，关键在于求出代价函数的导数，即：

<img src="http://latex.codecogs.com/svg.latex?\frac{\partial}{\partial\theta_{j}}J(\theta_{0}\theta_{1})=\frac{\partial}{\partial\theta_{j}} \sum_{2m}^1(h_{\theta}(x^{(i)})-y^{(i)})^2">

<img src="http://latex.codecogs.com/svg.latex?when\,j=0:\;\frac{\partial}{\partial\theta_{0}J(\theta_{0}\theta{1})}=\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})">

<img src="http://latex.codecogs.com/svg.latex?when\,j=1:\;\frac{\partial}{\partial\theta_{1}J(\theta_{0}\theta{1})}=\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)}).x_{(i)})">

则，梯度下降算法可以改写成：

repeat {

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})">

​	<img src="http://latex.codecogs.com/svg.latex?\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum_{i=1}^m((h_{\theta}(x^{(i)})-y^{(i)}).x_{(i)})">

}

上面使用的算法，有时也称为**批量梯度下降**。批量梯度下降是指，在梯度下降的每一步中，我们都用到了所有训练样本，在梯度下降中，在计算微分求导时，我们需要进行求和运算。所以，在每一个单独的梯度下降中，我们最终要计算该项对m个训练样本求和。

因此，批量梯度下降法说明了我们需要考虑所有这一批训练样本，而事实上，有其他类型的梯度下降法，不是这种批量型的，不考虑整个训练集，而每次只关注训练集中的一些小的子集。

**线性代数**中有一种计算代价函数 J 最小值的**数值解法**，不需要梯度下降这种迭代算法，这是另一种称为**正规方程**（noraml equations）的方法。实际上，在数据量较大的情况下，梯度下降法比正规方程更适用。



# 总结

本文介绍单变量线性回归模型，从模型表示，代价函数以及梯度下降等方面作介绍。当然，梯度下降法在机器学习问题方面应用相对广泛，后续将对不同的机器学习问题大量使用梯度下降法。



# 参考文献

[1] [吴恩达机器学习视频教程笔记](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes)